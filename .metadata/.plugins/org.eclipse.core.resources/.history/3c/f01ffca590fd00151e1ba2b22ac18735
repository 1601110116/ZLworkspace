/*
 * RunManager.cpp
 *
 *  Created on: Mar 6, 2016
 *      Author: zlstudio
 */

#include <MPIRunManager.h>

MPIGrid* MPIRunManager::grid=NULL;
double MPIRunManager::deltaT=0.0;
void (*MPIRunManager::Method)(const Range&, double)=NULL;
int MPIRunManager::MPI_ID=0;

static int n=0;
static double dt1=0;
static double dt2=0;

MPIRunManager::MPIRunManager(MPIGrid* _grid,void (*_Method)(const Range &,double),double _dt) {
	// TODO Auto-generated constructor stub
	grid=_grid;
	Method=_Method;
	deltaT=_dt;
}

MPIRunManager::~MPIRunManager() {
	// TODO Auto-generated destructor stub
}

void MPIRunManager::stepNext(){

	n++;

	double t=omp_get_wtime();

	if(grid->gridY()%omp_get_num_procs()!=0)cerr<<"Warning: Grid size does not match CPU architecture!"<<endl;

	int divide=grid->gridY()/omp_get_num_procs();

#pragma omp parallel
	{
		int i=omp_get_thread_num();

		Range r(2,grid->gridX()-2,i*divide,(i+1)*divide,0,grid->gridZ());

		Method(r,deltaT);
	}

	dt1+=(omp_get_wtime()-t)*1000;

	t=omp_get_wtime();

	grid->refreshParticleLocation();

	dt2+=(omp_get_wtime()-t)*1000;

	if(n>=10){
		cout<<"Time:\tCal\t"<<dt1/n<<" ms\tArr:\t"<<dt2/n<<" ms"<<endl;
		dt1=dt2=n=0;
	}

}

void MPIRunManager::Init_Prarllel(int &totalNodes,int &thisID){

	int l=30;
	char name[30];
	MPI_Comm_rank(MPI_COMM_WORLD,&thisID);
	MPI_Comm_size(MPI_COMM_WORLD,&totalNodes);
	MPI_Get_processor_name(name,&l);

	int prevNode=(thisID==0?totalNodes-1:thisID-1);
	int nextNode=(thisID==totalNodes-1?0:thisID+1);

	if(thisID==0)MPI_Send(&thisID,1,MPI_INT,nextNode,0,MPI_COMM_WORLD);
	int receive_msg;
	MPI_Status recv_state;
	MPI_Recv(&receive_msg,1,MPI_INT,prevNode,0,MPI_COMM_WORLD,&recv_state);

	cout <<"Node"<<thisID<<"/"<<totalNodes<<" at "<<name<<" has "<<omp_get_num_procs()<<" threads"<< endl;

	if(thisID!=0)MPI_Send(&thisID,1,MPI_INT,nextNode,0,MPI_COMM_WORLD);

	MPI_ID=thisID;

}

string MPIRunManager::toString(){
	ostringstream tmpchar;

	tmpchar<<"Node "<<MPI_ID<<endl;

	return tmpchar.str();
}

