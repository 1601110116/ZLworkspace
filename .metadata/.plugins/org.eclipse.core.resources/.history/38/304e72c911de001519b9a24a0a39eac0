#include <mpi.h>
#include <stdio.h>
#include <string.h>
#include <math.h>
#include <omp.h>
#include <lua.h>
#include <lauxlib.h>
#include <lualib.h>
#include <hdf5.h>

#include "push_particle.h"
#include "Miscellaneous.h"
#include "ListCell.h"
#include "BoundComm.h"
#include "DeclareInputs.h"
#include "input_lua.h"
#include "output_hdf5.h"
#include "mpidefs.h"



void set_uniform_A(Field3D_MPI_DOUBLE *vec);

double dT = 0.1, delta_grid = 0.0001, B0 = 10, *Pl0;

double uniform_dist(double imin,double imax){
	return imin+(imax-imin)*(rand()+.5)/(RAND_MAX+1.);
}

double maxwell_dist(double ava,double sig){
	return sig*sqrt(-2.*log(uniform_dist(0,1)))*cos(M_PI*2.*uniform_dist(0,1))+ava;
}

#define RANK 2

int main(int argc, char**argv){
	/* set stdout and stderr buffer to 0 */
	char stdoutbuff[160];
	char stderrbuff[160];
	//setvbuf(stdout, stdoutbuff, _IOLBF, 40);
	//setvbuf(stderr, stderrbuff, _IOLBF, 40);

	MPI_Init(&argc, &argv);
	#include "INPUT_TOC.h"
	Field3D_MPI_DOUBLE vectest;
	Field3D_MPI_DOUBLE vectest1;
	Field3D_MPI_DOUBLE J0;
	Field3D_MPI_DOUBLE A0;
	Field3D_MPI_DOUBLE A_all, *PA_all;
	int cur_rank;
	MPI_Comm_rank(MPI_COMM_WORLD, &cur_rank);
        MPI_Comm comm  = MPI_COMM_WORLD;
        MPI_Info mpi_info  = MPI_INFO_NULL;

	// Create h5 file
	hid_t fileID=CreateH5_file_Para(HDF5_OUTPUT_FILE,comm,mpi_info);
	// Create h5 group
	hid_t groupID=CreateH5_group(fileID, "Electric Field");
	
	//size_t DimInfo[RANK]={XMAX*YMAX*ZMAX*X_NUM_PROC*Y_NUM_PROC*Z_NUM_PROC,NUM_TIME_STEP};
	size_t DimInfo[RANK]={XMAX*X_NUM_PROC*Y_NUM_PROC*Z_NUM_PROC,NUM_TIME_STEP};
	hid_t DataType = H5T_IEEE_F64LE;//H5T_IEEE_F64LE  H5T_NATIVE_INT		
	hid_t datasetID_Ex = CreateH5_dataset(groupID,"Ex",DataType, RANK,DimInfo); 
	hid_t datasetID_Ey = CreateH5_dataset(groupID,"Ey",DataType, RANK,DimInfo); 
	hid_t datasetID_Ez = CreateH5_dataset(groupID,"Ez",DataType, RANK,DimInfo); 
	hid_t datasetID_N = CreateH5_dataset(groupID,"Num",DataType, RANK,DimInfo); 
	
	omp_set_num_threads(1);
	vec_init_mpi(&vectest, MPI_COMM_WORLD, X_NUM_PROC, Y_NUM_PROC, Z_NUM_PROC, XMAX, YMAX, ZMAX, 6, OVLP);
	vec_init_mpi(&J0, MPI_COMM_WORLD, X_NUM_PROC, Y_NUM_PROC, Z_NUM_PROC, XMAX, YMAX, ZMAX, 3, OVLP);
	vec_init_mpi_from(&vectest1, &vectest);
	vec_init_mpi_from(&A0, &J0);
	vec_init_mpi_from(&A_all, &A0);
	vec_set_zero(&(vectest.localvec));
	vec_set_zero(&(J0.localvec));
	
	
	
	//printf("now $%d out of %d processes are running.\n", cur_rank, 2);
	double beg = MPI_Wtime();
	int tn;
	Field3D_MPI_DOUBLE * p0 = &vectest, *p1 = &vectest1;
	
	// Initial Extern field
	NUMBER_REAL Time=0;
	if(0){
		double ip[4]={10,2,0,0};
		double op[3];
		double op1[3];
		LuaTOC_func(L,ExternField,4,ip,3,op);
		LuaTOC_func(L,ExternField,4,ip,3,op1);
		fprintf(stderr,"%e %e %e\n",op1[0],op1[1],op1[2]);
		MPI_Abort(MPI_COMM_WORLD,0);
	}
	if (Open_Ex_A0==1)
	{
		PA_all=&A_all;
		Merge_Field_A(L,ExternField,Time,p0,&A0,PA_all);
		//fprintf(stderr,"PA_all=0x%lx\n",PA_all);
	}
	else
	{
		PA_all=p0;
	}
	// Initial Extern field--END
	
	particlesInCells mypic;
	double x03[6] = { 0 + p0->cur_rank*XMAX, 0, 0, XMAX + p0->cur_rankx*XMAX, YMAX, ZMAX };
	PIC_SSIZE_T iNs[3] = { XMAX, YMAX, ZMAX };
	initialCellsByNs(x03, x03 + 3, iNs, &mypic);
	srand(cur_rank+1);
	//fprintf(stderr,"rank=%d,rand=%d\n",cur_rank,rand());
	{
		//int x;
		//for(x=0;x<XMAX;x++){
		fffor(XMAX,YMAX,ZMAX)
			int p;
			int ppg=PPG-AMPE*cos(2*M_PI/(XMAX*X_NUM_PROC)*(.5+xyzx+p0->cur_rank*XMAX))*2*M_PI/(XMAX*X_NUM_PROC)/ELEC_PER_SAMP;
			//ppg=PPG;
			for(p=0;p<ppg;p++){
				double xp0[6];
				//NUMBER_REAL tempV[3]={0.1,0.1,0.1},tempP[3];
				//NUMBER_REAL tempV[3]={maxwell_dist(0,VT),0,0},tempP[3];
				//NUMBER_REAL tempX[3]={uniform_dist(x+cur_rank*XMAX,x+1+cur_rank*XMAX),uniform_dist(0,1),uniform_dist(0,1)};
				NUMBER_REAL tempX[3]={uniform_dist(xyzx+p0->cur_rankx*XMAX,xyzx+1+p0->cur_rankx*XMAX),uniform_dist(xyzy,xyzy+1),uniform_dist(xyzz,xyzz+1)};
				NUMBER_REAL tempV[3]={maxwell_dist(0,VT),maxwell_dist(0,VT),maxwell_dist(0,VT)},tempP[3];
				if(1){
					TransV2P(PA_all,type_weight,tempX,tempV,MASS,CHARGE,tempP);
				}

				int i;
				for(i=0;i<6;i++)
				{
					if(i<3)
					{	
						xp0[i]=tempX[i];
					}
					else
					{	
						xp0[i]=tempP[i-3];
						//xp0[i]=tempV[i-3];
					}
				}
				addParticle(xp0, &mypic);
			}
			vec_get_grid_pointer(p0,xyzx,xyzy,xyzz)[3]+=AMPE*sin(2*M_PI/(XMAX*X_NUM_PROC)*(xyzx+p0->cur_rankx*XMAX));
		 endfff
	}
	Parameter4BoudSet parameter4boundset;
	parameter4boundset.LState=L;
	parameter4boundset.funcname=ExternField;
	parameter4boundset.charge=CHARGE;
	BoundComm mybound;
	initialBoundComm(&mybound, &mypic, p0, MPI_COMM_WORLD);
	setupOffset(Reset_Boundary_data,&parameter4boundset,&mybound);
	int ia, ib;
	MPI_Barrier(MPI_COMM_WORLD);
	//printf("#%d: I am ready to loop!\n", cur_rank);
	for (tn = 0; tn < NUM_TIME_STEP; tn++){
		Time = tn*DeltaT;
		parameter4boundset.Time=Time;
		sync_overlap(p0);
	//Set Extern field	
		if(Open_Ex_A0){
			Merge_Field_A(L,ExternField,Time,p0,&A0,PA_all);
		}else{
			PA_all=p0;
		}
	//Set Extern field--END	

		if (cur_rank == 0){
			vec_get_grid_pointer(p0, 0, 0, 0)[3] += 0 * sin(0.1*tn);
		}
		if(0&&SAVE_DATA){
			NUMBER_REAL ExBuffer[XMAX],NBuffer[XMAX];
			NUMBER_REAL EyBuffer[XMAX],EzBuffer[XMAX];
			size_t Offset[RANK]={p0->cur_rankx*XMAX,tn};
			size_t BufDims[RANK]={XMAX,1};
			int i;
			//fprintf(stderr,"rk=%d,p0=%e\n",cur_rank,vec_get_element_mpi(p0,10,0,0,0));
			for(i=0;i<XMAX;i++){
				PIC_SSIZE_T xgrid[3]={i,0,0};
				NBuffer[i]= get_ppg(&mypic,xgrid);
				ExBuffer[i]=vec_get_element_mpi(p0,i,0,0,3);
				EyBuffer[i]=vec_get_element_mpi(p0,i,0,0,4);
				EzBuffer[i]=vec_get_element_mpi(p0,i,0,0,5);
			}
			WriteData_Para(datasetID_Ex,DataType,RANK,BufDims,Offset,ExBuffer);
			WriteData_Para(datasetID_Ey,DataType,RANK,BufDims,Offset,EyBuffer);
			WriteData_Para(datasetID_Ez,DataType,RANK,BufDims,Offset,EzBuffer);
			WriteData_Para(datasetID_N,DataType,RANK,BufDims,Offset,NBuffer);
		}
		//printf("#%d: before sync_overlap.\n", cur_rank);
		//sync_overlap(PA_all);
		//printf("#%d: end sync_overlap.\n", cur_rank);

		// Particle Count Subroutine.
		if(0){
			PIC_SSIZE_T numParticleInNodes[X_NUM_PROC];
			resetIterator(&mypic);
			numParticleInNodes[0] = 0;
			while(getNextParticle(&mypic) != NULL){
				numParticleInNodes[0]++;
			}
			if (cur_rank == 0){
				PIC_SSIZE_T idproc, totalcount = 0;
				MPI_Status tempstat;
				//Ouput the number of each node and the sum.
				for(idproc = 1; idproc < X_NUM_PROC; ++idproc){
					MPI_Recv(numParticleInNodes+idproc, 1, MPI_INT, idproc, 10+idproc, MPI_COMM_WORLD, &tempstat);
				}
				printf("@t = %d: ",tn);
				for(idproc = 0; idproc < X_NUM_PROC; ++idproc){
					printf("%d, ", numParticleInNodes[idproc]);
					totalcount += numParticleInNodes[idproc];
				}
				printf("Sum = %d\n", totalcount);
				fflush(stdout);
			}else{
				MPI_Send(numParticleInNodes, 1, MPI_INT, 0, 10+cur_rank, MPI_COMM_WORLD);
			}
		}
		if(cur_rank==0){
			printf("@t = %d: \n",tn);
		}

		// The following is done by both.
		vec_set_zero(&(J0.localvec));
		sync_overlap(&J0);
		PIC_SSIZE_T l;
		
		for (l = 0; l < totalLatticeNum(&mypic.rootsLattice); l++){
			PIC_SSIZE_T X_grid[3] = { 0, 0, 0 };
			index2read(&l, X_grid, &mypic.rootsLattice);
			//fprintf(stderr,"%d ",l);
			CanonSymp_Particle_Pusher(&mypic, PA_all, &J0, type_weight, X_grid, CHARGE,MASS,ELEC_PER_SAMP,DeltaT,tn);
		}
		merge_overlap(&J0);
		//printf("#%d: End merge_overlap.\n", cur_rank);
		//vec_set_zero(&(J0.localvec));
		if(SAVE_DATA){
			NUMBER_REAL ExBuffer[XMAX],NBuffer[XMAX];
			NUMBER_REAL EyBuffer[XMAX],EzBuffer[XMAX];
			size_t Offset[RANK]={p0->cur_rankx*XMAX,tn};
			size_t BufDims[RANK]={XMAX,1};
			int i;
			//fprintf(stderr,"rk=%d,p0=%e\n",cur_rank,vec_get_element_mpi(p0,10,0,0,0));
			for(i=0;i<XMAX;i++){
				PIC_SSIZE_T xgrid[3]={i,0,0};
				NBuffer[i]= get_ppg(&mypic,xgrid);
				ExBuffer[i]=vec_get_element_mpi(p0,i,0,0,3);
				EyBuffer[i]=vec_get_element_mpi(p0,i,0,0,4);
				EzBuffer[i]=vec_get_element_mpi(p0,i,0,0,5);
			}
			WriteData_Para(datasetID_Ex,DataType,RANK,BufDims,Offset,ExBuffer);
			WriteData_Para(datasetID_Ey,DataType,RANK,BufDims,Offset,EyBuffer);
			WriteData_Para(datasetID_Ez,DataType,RANK,BufDims,Offset,EzBuffer);
			WriteData_Para(datasetID_N,DataType,RANK,BufDims,Offset,NBuffer);
		}
		Canonical_A_Y_One_Step(p1, p0, &J0, DeltaT);
		//printf("#%d: The begin of updateCells\n", cur_rank);
		//fflush(stdout);
		MPI_updateCells(&mypic, &mybound, MPI_COMM_WORLD);
		//printf("#%d: The end of updateCells\n", cur_rank);
		//fflush(stdout);
		//MPI_Barrier(MPI_COMM_WORLD);

		Field3D_MPI_DOUBLE * tmp = p0;
		p0 = p1;
		p1 = tmp;
	}
	vec_destroy_mpi(&vectest);
	vec_destroy_mpi(&vectest1);

	destroyBoundComm(&mybound);
	destroyCells(&mypic);
	hid_t did[]={datasetID_Ex,datasetID_N,datasetID_Ey,datasetID_Ez};
	CloseH5_FGD(fileID,&groupID,1,did,4);
	//CloseH5_FGD(fileID,groupID,datasetID_N);
	
	MPI_Finalize();
}
